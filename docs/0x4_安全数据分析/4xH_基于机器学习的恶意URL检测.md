# åŸºäºæœºå™¨å­¦ä¹ çš„æ¶æ„URLæ£€æµ‹

## ç›®å½•

[TOC]



## æ¦‚è¿°

æœ¬æ¬¡å®è·µè™½é¢˜ä¸ºâ€åŸºäºæœºå™¨å­¦ä¹ â€œï¼Œä½†ç›®å‰ä¹Ÿåªæ›´æ–°åˆ°ä½¿ç”¨TF-IDFæå–ç‰¹å¾ï¼Œä½¿ç”¨é€»è¾‘å›å½’å’ŒSVMæ¨¡å‹çš„è¿›åº¦ã€‚ä¸è¿‡åƒé‡Œä¹‹è¡Œå§‹äºè¶³ä¸‹å˜›ï¼Œåé¢å†æ›´å•¦ğŸ¦



## æå‡ºé—®é¢˜

é¦–å…ˆæˆ‘ä»¬éœ€è¦å°†å®‰å…¨é—®é¢˜è¿›è¡ŒæŠ½è±¡ï¼Œä¹Ÿå°±æ˜¯é’ˆå¯¹ç°çŠ¶æå‡ºé—®é¢˜ã€‚åœ¨å‡å®šçš„è¿™ä¸ªä¸šåŠ¡èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬å‘ç°ï¼š

-   æ¶æ„URLå­˜åœ¨ç‰¹å®šå‡ ç§ç±»å‹1

-   ç‰¹å®šç±»å‹æ¶æ„URLåœ¨æ–‡æœ¬ä¸Šå­˜åœ¨æ™®éçš„**è¯æ±‡ç‰¹å¾**ï¼Œä¾‹å¦‚é’“é±¼URLä¸­å¸¸è§"login", "account", "sigin"ç­‰å…³é”®è¯

å› æ­¤æˆ‘ä»¬å°è¯•ä½¿ç”¨æœºå™¨å­¦ä¹ ç®—æ³•å¯¹æ¶æ„URLè¿›è¡Œæ£€æµ‹åˆ†æã€‚



## æ•°æ®å¤„ç†

### æ ·æœ¬é€‰æ‹©

-   [malicious-URLs](https://github.com/faizann24/Using-machine-learning-to-detect-malicious-URLs)
    -   **malicious-URLs** åœ¨Githubä¸Šé¢ä¸€ä¸ª ä½¿ç”¨æœºå™¨å­¦ä¹ å»æ£€æµ‹æ¶æ„URLçš„é¡¹ç›® ï¼Œé‡Œé¢æœ‰ä¸€ä¸ªè®­ç»ƒé›†ï¼Œæœ‰åšæ ‡è®°æ˜¯æ­£å¸¸çš„URLè¿˜æ˜¯æ¶æ„çš„URL
    -   å†…å®¹ç±»å‹ï¼šæ–‡æœ¬æ ·æœ¬
    -   æ˜¯å¦æ ‡è®°ï¼šæ˜¯
    -   æ˜¯å¦ç‰¹å¾åŒ–ï¼šå¦
    -   ä½¿ç”¨èŒƒå›´ï¼šå…¥ä¾µæ£€æµ‹ã€å¼‚å¸¸æµé‡ã€WAF

### æ•°æ®æ¸…æ´—

ç”±äºæ ·æœ¬æœ¬èº«ä¸ºå¤„ç†å¥½çš„æ ‡è®°æ•°æ®ï¼Œæ‰€ä»¥åœ¨æ•°æ®æ ¼å¼å’Œè„æ•°æ®ä¸Šæ— éœ€å¤„ç†ï¼ˆçœŸå®æƒ…å†µä¸‹å¯èƒ½æ­£å¥½ç›¸å:-(ï¼‰ã€‚

ç¼–å†™æ•°æ®å¸§æå–å‡½æ•°ï¼š

```python
def csv_data_read(csv_file_path):
    # ä¸ºå‡å°‘è®­ç»ƒæ—¶é—´ï¼Œå¯åªå–å¤´éƒ¨10Wæ¡ï¼Œä½†ä¸€å®šéœ€è¦å…ˆæ‰“ä¹±æ ·æœ¬ï¼‰
    # df_csv = pd.read_csv(csv_file_path).head(100000)
    df_csv = pd.read_csv(csv_file_path)
    urls = []
    labels = []
    for index, row in df_csv.iterrows():
        urls.append(row["url"])
        labels.append(row["label"])
    return urls, labels
```

ç¼–å†™å¯¹URLçš„æ•°æ®æ¸…æ´—å‡½æ•°ï¼š
```python
def url_tokenize(url):
    """
    å¯¹URLè¿›è¡Œæ¸…æ´—ï¼Œåˆ é™¤æ–œçº¿ã€ç‚¹ã€å’Œcomï¼Œè¿›è¡Œåˆ†è¯
    :param url:
    :return:
    """
    web_url = url.lower()
    dot_slash = []
    slash = str(web_url).split('/')
    for i in slash:
        r1 = str(i).split('-')
        token_slash = []
        for j in range(0,len(r1)):
            r2 = str(r1[j]).split('.')
            token_slash = token_slash + r2
        dot_slash = dot_slash + r1 + token_slash
    urltoken_list = list(set(dot_slash))
    white_words = ["com", "http:", "https:", ""]
    for white_word in white_words:
        if white_word in urltoken_list:
            urltoken_list.remove(white_word)
    return urltoken_list
```



## ç‰¹å¾æå–

æˆ‘ä»¬é¦–å…ˆåŠ è½½æ•°æ®é›†ï¼š

```python
    grep_csv_file_path = "../../data/data-0x3/grey-url.csv"
    black_csv_file_path = "../../data/data-0x3/black-url.csv"
    grey_urls, y = csv_data_read(grep_csv_file_path)
```

æˆ‘ä»¬ä½¿ç”¨TF-IDFç®—æ³•æå–URLçš„ç‰¹å¾ï¼Œå¹¶å°†æ•°æ®å¸§åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

```python
    url_vectorizer = TfidfVectorizer(tokenizer=url_tokenize)
    x = url_vectorizer.fit_transform(grey_urls)
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
```

### æ³¨ï¼šTF-IDF

TF-IDF(Term Frequency â€“ Inverse Document Frequency)ï¼Œå³è¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡ã€‚åœ¨è®¡ç®—ä¸Šä¸ºè¯é¢‘å’Œé€†æ–‡æ¡£é¢‘ç‡çš„ä¹˜ç§¯ã€‚è®¡ç®—æ–¹æ³•å¦‚ä¸‹ï¼š

-   è®¡ç®—è¯é¢‘ï¼ˆTFï¼‰
    -   æŸä¸ªè¯åœ¨æ–‡ç« ä¸­å‡ºç°çš„æ¬¡æ•°/æ–‡ç« çš„æ€»è¯æ•°
    -   å³æŸä¸ªè¯åœ¨è¿™æ®µæ–‡å­—ä¸­å‡ºç°å¾—è¶Šå¤šï¼ŒTFå°±è¶Šå¤§
-   è®¡ç®—é€†æ–‡æ¡£é¢‘ç‡ï¼ˆIDFï¼‰
    -   log(è¯­æ–™åº“çš„æ–‡æ¡£æ€»æ•°/åŒ…å«è¯¥è¯çš„æ–‡æ¡£æ•°+1)
    -   æŸä¸ªè¯åœ¨æ™®éæƒ…å†µä¸‹è¶Šå¸¸è§ï¼Œåˆ†æ¯å¤§ï¼ŒIDFä¹Ÿçº¦è¶‹äº0
-   è®¡ç®—TF-IDF
    -   TF-IDF = TF * IDF
    -   TF-IDFè¶Šå¤§ï¼Œè¯´æ˜è¯åœ¨è¿™æ®µæ–‡ç« ä¸­è¶Šé‡è¦ï¼Œä½†å› ä¸ºæœ‰IDFçš„å­˜åœ¨ï¼Œåˆèƒ½é¿å…æŠŠâ€œæ˜¯â€ã€â€çš„â€œã€â€œå’Œâ€ç­‰åœç”¨è¯çš„TF-IDFå€¼é™ä½

åœ¨åº”ç”¨ä¸Šï¼šå°†æ–‡ç« åˆ†è¯ï¼Œè®¡ç®—TF-IDFï¼ŒæŒ‰ç…§å…¶å€¼å¤§å°é™åºæ’åˆ—ï¼Œæ’åé å‰çš„å³æ–‡ç« çš„å…³é”®è¯



## æ¨¡å‹é€‰æ‹©

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯¹æ•°æ®é›†ä½¿ç”¨**é€»è¾‘å›å½’æ¨¡å‹**ï¼Œå¹¶å°†å°†æ‹Ÿåˆåçš„æ¨¡å‹å’Œå‘é‡ä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶ï¼Œä¾¿äºé‡å¤ä½¿ç”¨

```python
    # å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ‰§è¡Œé€»è¾‘å›å½’
    l_regress = LogisticRegression(solver='liblinear')
    l_regress.fit(x_train, y_train)
    l_score = l_regress.score(x_test, y_test)
    # print("æµ‹è¯•æ‹Ÿåˆåˆ†æ•°ä¸ºï¼š{0}".format(l_score))

    file_mode = "../../model/model-0x3/model.pkl"
    dump_model_object(file_mode)
    file_vector = "../../model/model-0x3/vector.pl"
    dump_model_object(file_vector)
```

æ­¤å¤–ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨æ”¯æŒå‘é‡æœºæ¨¡å‹ï¼š

```python
def practice_svm(x_train, x_test, y_train, y_test):
    """
    å®è·µSVMç®—æ³•è¯†åˆ«æ¶æ„URL
    :param x_train:
    :param x_test:
    :param y_train:
    :param y_test:
    :return:
    """
    model_svm = SVC()
    # æ³¨æ„ï¼šSVMè®­ç»ƒå¯èƒ½è¾ƒæ…¢ï¼Œæ³¨æ„æ ·æœ¬çš„æ•°é‡
    model_svm.fit(x_train, y_train)
    svm_score = model_svm.score(x_test, y_test)
    print("æµ‹è¯•æ‹Ÿåˆåˆ†æ•°ä¸ºï¼š{0}".format(svm_score))
    model_svm_save = model_svm

    """
    ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å’Œå‘é‡
    """
    file_mode = "../../model/model-0x3/model_svm.pkl"
    dump_model_object(file_mode, model_svm_save)
```





## æ•ˆæœè¯„ä¼°

åœ¨â€œç‰¹å¾æå–â€éƒ¨åˆ†æˆ‘ä»¬é‡‡ç”¨`train_test_split`æ–¹æ³•è¿›è¡Œéšæœºåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œè¿›è¡Œ**äº¤å‰éªŒè¯**ï¼Œå†æ¬¡å›é¡¾ä»£ç ä¸ºï¼š

```python
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
```

ä½¿ç”¨çº¿æ€§å›å½’æ¨¡å‹ï¼Œæœ€åå¾—åˆ°æµ‹è¯•æ‹Ÿåˆåˆ†æ•°ä¸ºï¼š**0.9599966703530615**

æ³¨ï¼Œå‚æ•°ä»‹ç»ï¼š

-   `test_size`ï¼šæµ‹è¯•é›†åœ¨æ€»æ ·æœ¬ä¸­çš„å æ¯”
-   `random_state`ï¼šéšæœºæ•°çš„ç§å­ï¼Œä¹Ÿå¯ä»¥ç†è§£ä¸ºè¯¥ç»„éšæœºæ•°çš„ç¼–å·ã€‚è§„åˆ™æ˜¯ï¼šç§å­ä¸åŒæ—¶ï¼Œäº§ç”Ÿä¸åŒçš„éšæœºæ•°ï¼›ç§å­ç›¸åŒæ—¶ï¼Œåœ¨ä¸åŒå®ä¾‹ä¸‹ä¹Ÿäº§ç”Ÿç›¸åŒçš„éšæœºæ•°ã€‚æ¯”å¦‚åœ¨ä¸Šé¢çš„è¯­å¥ä¸­ï¼Œ`test_size`ä¸º0.2ï¼Œå³é€‰æ‹©æ€»æ ·æœ¬çš„20%ä½œä¸ºæµ‹è¯•é›†ï¼Œä½†æ˜¯å¦‚ä½•é€‰æ‹©å‘¢ï¼Ÿ`random_state`å°±æŒ‡å®šäº†ï¼šæŒ‰ç…§â€œç¬¬42ç§â€è§„åˆ™é€‰æ‹©è¿™20%éšæœºçš„æ•°æ®ã€‚

ä½¿ç”¨æ”¯æŒå‘é‡æœºæ¨¡å‹æ—¶ï¼Œå‘ç°åœ¨æ•°æ®é‡è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹çš„è¿ç®—é€Ÿåº¦è¾ƒæ…¢ï¼Œå› æ­¤åœ¨å®éªŒç¯å¢ƒä¸‹ä¸å¾—å·²å‡å°‘äº†è®­ç»ƒæ ·æœ¬çš„æ•°é‡ï¼Œä½†ä¹Ÿå¯¼è‡´äº†æ‹Ÿåˆåˆ†æ•°çš„é™ä½ï¼Œæ‰€ä»¥å°±ä¸å±•ç¤ºåœ¨æ ·æœ¬ç¼©æ°´æƒ…å†µä¸‹çš„æµ‹è¯•æ‹Ÿåˆåˆ†æ•°äº†ã€‚åŒæ—¶ï¼Œä¹Ÿäº†è§£åˆ°è¿™æ˜¯ä¼ ç»ŸäºŒåˆ†ç±»SVMåœ¨é¢å¯¹å¤§æ•°æ®é‡æ—¶çš„å¼Šç«¯ï¼Œå¹¶ä¸”éšç€é›†æˆå­¦ä¹ çš„æˆç†Ÿï¼ŒSVMç°åœ¨â€œæ™®éç”¨äºé›†æˆå­¦ä¹ ä¸­åŸºæ¨¡å‹çš„æ„å»ºâ€[2]ï¼Œè€Œä¸æ˜¯ä½œä¸ºå”¯ä¸€çš„åˆ†ç±»æ¨¡å‹ä½¿ç”¨ã€‚





## å®Œæ•´ä»£ç 

```python
"""
Author: Toky
Description: åŸºäºæœºå™¨å­¦ä¹ çš„æ¶æ„URLæ£€æµ‹
"""
import copy
import pickle

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression


def csv_data_read(csv_file_path):
    # ä¸ºå‡å°‘è®­ç»ƒæ—¶é—´ï¼Œå¯åªå–å¤´éƒ¨10Wæ¡ï¼Œä½†ä¸€å®šéœ€è¦å…ˆæ‰“ä¹±æ ·æœ¬ï¼‰
    # df_csv = pd.read_csv(csv_file_path).head(100000)
    df_csv = pd.read_csv(csv_file_path)
    urls = []
    labels = []
    for index, row in df_csv.iterrows():
        urls.append(row["url"])
        labels.append(row["label"])
    return urls, labels


def url_tokenize(url):
    """
    å¯¹URLè¿›è¡Œæ¸…æ´—ï¼Œåˆ é™¤æ–œçº¿ã€ç‚¹ã€å’Œcomï¼Œè¿›è¡Œåˆ†è¯
    :param url:
    :return:
    """
    web_url = url.lower()
    dot_slash = []
    slash = str(web_url).split('/')
    for i in slash:
        r1 = str(i).split('-')
        token_slash = []
        for j in range(0,len(r1)):
            r2 = str(r1[j]).split('.')
            token_slash = token_slash + r2
        dot_slash = dot_slash + r1 + token_slash
    urltoken_list = list(set(dot_slash))
    white_words = ["com", "http:", "https:", ""]
    for white_word in white_words:
        if white_word in urltoken_list:
            urltoken_list.remove(white_word)
    return urltoken_list


def dump_model_object(file_path, model_object):
    """
    ä½¿ç”¨pickleå°†å†…å­˜ä¸­çš„å¯¹è±¡è½¬æ¢ä¸ºæ–‡æœ¬æµä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶
    :param file_path:
    :return:
    """
    with open(file_path, "wb") as f:
        pickle.dump(model_object, f)
    f.close()


if __name__ == '__main__':
    """
    åŠ è½½æ•°æ®é›†
    """
    grep_csv_file_path = "../../data/data-0x3/grey-url.csv"
    black_csv_file_path = "../../data/data-0x3/black-url.csv"
    grey_urls, y = csv_data_read(grep_csv_file_path)

    """
    ä½¿ç”¨TF-IDFç®—æ³•æå–å…³é”®è¯ç‰¹å¾ï¼Œå¹¶å°†æ•°æ®å¸§åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
    """
    url_vectorizer = TfidfVectorizer(tokenizer=url_tokenize)
    url_vectorizer_save = copy.deepcopy(url_vectorizer)
    x = url_vectorizer.fit_transform(grey_urls)
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

    """
    å¯¹æ•°æ®å¸§æ‰§è¡Œé€»è¾‘å›å½’ï¼Œå°†æ‹Ÿåˆåçš„æ¨¡å‹å’Œå‘é‡ä¿å­˜
    """
    # å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ‰§è¡Œé€»è¾‘å›å½’
    l_regress = LogisticRegression(solver='liblinear')
    l_regress.fit(x_train, y_train)
    l_score = l_regress.score(x_test, y_test)
    print("æµ‹è¯•æ‹Ÿåˆåˆ†æ•°ä¸ºï¼š{0}".format(l_score))

    file_mode = "../../model/model-0x3/model.pkl"
    dump_model_object(file_mode, l_regress)
    file_vector = "../../model/model-0x3/vector.pl"
    dump_model_object(file_vector, url_vectorizer_save)
    
```



##  æ¦‚å¿µè¡¥å……

### é€»è¾‘å›å½’

é€»è¾‘å›å½’æ¨¡å‹é€šè¿‡é€»è¾‘å‡½æ•°å¯¹æ•°æ®è¿›è¡Œ**åˆ†ç±»**ï¼Œé€šå¸¸åŒ…æ‹¬ç”¨äºä¼°è®¡é€»è¾‘æ¨¡å‹ç»“æœçš„ç‹¬ç«‹äºŒå…ƒå˜é‡ã€‚ç›¸æ¯”äºçº¿æ€§å›å½’ï¼Œé€»è¾‘å›å½’å¤„ç†çš„åˆ†ç±»é—®é¢˜ï¼Œè¾“å‡ºçš„ç»“æœä¸ºç¦»æ•£å€¼ï¼›è€Œçº¿æ€§å›å½’è§£å†³çš„æ˜¯å›å½’é—®é¢˜è¾“å‡ºçš„æ˜¯è¿ç»­å€¼ã€‚

è¯¦ç»†è§£è¯»å¯ä»¥å‚è€ƒæ–‡ç« [1]ï¼Œæ³¨æ„ï¼Œè™½ç„¶è¯¥ç®—æ³•åœ¨ç”¨èµ·æ¥æ—¶æ˜¾å¾—éå¸¸ç®€å•ï¼Œä½†æ˜¯å…¶åŸç†ä¸­çš„ç»†èŠ‚éƒ¨åˆ†è¿˜æ˜¯å¾ˆå¤šçš„ï¼Œæ„Ÿå…´è¶£å¯ä»¥ä»”ç»†ç ”ç©¶ä¸€ä¸‹ã€‚



### æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰

æ”¯æŒå‘é‡æœºï¼ˆSurport Vector Machine, SVMï¼‰åŒæ ·ç”¨äºåˆ†ç±»ï¼Œæ˜¯ä¸€ä¸ªäºŒå…ƒåˆ†ç±»ç®—æ³•ï¼Œä½†ä¿®æ”¹åä¹Ÿæ”¯æŒå¤šåˆ†ç±»é—®é¢˜ã€‚æ”¯æŒå‘é‡æœºé€šè¿‡åœ¨é«˜ç»´ç©ºé—´ä¸­åˆ›å»ºæœ€ä½³è¶…å¹³é¢æ¥å®ç°ï¼Œè¿™ä¸ªè¶…å¹³é¢åˆ›å»ºçš„åˆ’åˆ†è¢«ç§°ä¸ºç±»ã€‚

å¯¹äºåˆ†ç±»é—®é¢˜æœ¬è´¨çš„ç†è§£ï¼Œå°±æ˜¯æˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸€ä¸ªåˆ’åˆ†çš„è¶…å¹³é¢ï¼Œè®©æ•°æ®å°½å¯èƒ½å¤šåœ°åˆ†å¸ƒåœ¨è¿™ä¸ªå¹³é¢çš„ä¸¤ä¾§ï¼Œä»è€Œå®ç°åˆ†ç±»çš„æ•ˆæœã€‚ä½†åœ¨å®é™…æ•°æ®ä¸‹ï¼Œå¾€å¾€å­˜åœ¨å¤šä¸ªè¶…å¹³é¢ï¼Œé‚£ä¹ˆæ­¤æ—¶æˆ‘ä»¬æ€ä¹ˆå–èˆå‘¢ï¼Ÿå°±æ˜¯æ¯”è¾ƒå®¹æ˜“åˆ†ç±»é”™è¯¯çš„æ•°æ®ç‚¹ï¼Œè€Œè¿™äº›ç‚¹å°±æ˜¯ç¦»å¹³é¢å¾ˆè¿‘çš„ç‚¹ï¼Œå› ä¸ºç¦»å¹³é¢å¾ˆè¿œçš„ç‚¹æ˜¯ç›¸å·®å¾ˆå¤§çš„ï¼ŒåŸºæœ¬ä¸ä¼šå­˜åœ¨åˆ†ç±»é”™è¯¯çš„æƒ…å†µã€‚è€ŒSVMçš„æ ¸å¿ƒæ€æƒ³å°±æ˜¯å¦‚æ­¤ï¼Œæ‰¾åˆ°ç¦»å¹³é¢å¾ˆè¿‘çš„ã€å®¹æ˜“åˆ†ç±»é”™è¯¯çš„ç‚¹ï¼Œç„¶åæƒ³åŠæ³•è®©è¿™äº›æ•°æ®ç‚¹ç¦»å¹³é¢è·ç¦»å˜è¿œã€‚é‚£äº›ç¦»è¶…å¹³é¢å¾ˆè¿‘çš„ç‚¹ä¹Ÿå°±è¢«ç§°ä¸ºæ”¯æŒå‘é‡ï¼ˆSupport Vectorï¼‰ã€‚

è¯¦ç»†çš„æ•°å­¦åŸç†ï¼Œå¯ä»¥å‚è€ƒæ–‡ç« [2]ï¼Œè¯¥ç®—æ³•æœ‰æ¯”è¾ƒå®Œå¤‡çš„æ•°å­¦ç†è®ºæ”¯æ’‘çš„ï¼Œä½†è¯¦ç»†çš„æ•°ç†å’Œæ¨å€’ä¹Ÿç›¸å¯¹æ¯”è¾ƒå¤æ‚ï¼Œå› æ­¤ä¹Ÿå¯ä»¥çœ‹è‡ªå·±éœ€è¦è¿›è¡Œå­¦ä¹ ï¼ˆ~~å…¶å®å°±æ˜¯æˆ‘çœ‹ä¸æ‡‚ï¼Œä¸çŒ®ä¸‘æ¥æ¨å¯¼äº†~~ï¼‰ã€‚





## Reference

\[1]ã€æœºå™¨å­¦ä¹ ã€‘é€»è¾‘å›å½’ï¼ˆéå¸¸è¯¦ç»†ï¼‰ï¼Œ[é˜¿æ³½](https://www.zhihu.com/people/is-aze)ï¼Œhttps://zhuanlan.zhihu.com/p/74874291

\[2] 05 SVM - æ”¯æŒå‘é‡æœº - æ¦‚å¿µã€çº¿æ€§å¯åˆ†ï¼Œ[ç™½å°”æ‘©æ–¯](https://www.jianshu.com/u/a9f6de37f77b)ï¼Œhttps://www.jianshu.com/p/410a56129757